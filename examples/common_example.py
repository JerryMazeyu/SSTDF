"""
CommonæœåŠ¡æ¨¡å—æµ‹è¯•ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ app.services.common ä¸­çš„å„ç§æ¥å£
"""

import sys
import os
import time
import torch
import torch.nn as nn

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

try:
    from app.services.common import (
        model_registry,
        get_system_info,
        get_resource_usage,
        get_gpu_info,
        resource_monitor_stream,
        get_model_statistics,
        load_model_from_path,
        ModelInfo
    )
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"å½“å‰Pythonè·¯å¾„: {sys.path[:3]}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


def test_model_registry():
    """æµ‹è¯•æ¨¡å‹æ³¨å†Œå™¨åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•æ¨¡å‹æ³¨å†Œå™¨åŠŸèƒ½")
    print("=" * 60)
    
    # 1. æ³¨å†Œæ¨¡å‹
    print("\n1. æ³¨å†Œæ¨¡å‹:")
    models_to_register = [
        ("YOLOv5-Small", "/models/yolov5s.pt", "pytorch"),
        ("ResNet18", "/models/resnet18.pth", "pytorch"),
        ("EfficientNet", "/models/efficientnet.onnx", "onnx"),
        ("MobileNet", "/models/mobilenet.pt", "pytorch"),
    ]
    
    for name, path, framework in models_to_register:
        success = model_registry.register_model(name, path, framework)
        print(f"  æ³¨å†Œ {name}: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # 2. åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
    print("\n2. å·²æ³¨å†Œçš„æ¨¡å‹åˆ—è¡¨:")
    models = model_registry.list_models()
    for model in models:
        print(f"  - {model['name']} ({model['framework']}) - çŠ¶æ€: {model['status']}")
        print(f"    è·¯å¾„: {model['path']}")
        if model['device']:
            print(f"    è®¾å¤‡: {model['device']}")
    
    # 3. è·å–ç‰¹å®šæ¨¡å‹ä¿¡æ¯
    print("\n3. è·å–ç‰¹å®šæ¨¡å‹ä¿¡æ¯:")
    model_name = "YOLOv5-Small"
    model_info = model_registry.get_model_info(model_name)
    if model_info:
        print(f"  æ¨¡å‹åç§°: {model_info['name']}")
        print(f"  æ¨¡å‹è·¯å¾„: {model_info['path']}")
        print(f"  æ¡†æ¶: {model_info['framework']}")
        print(f"  çŠ¶æ€: {model_info['status']}")
    else:
        print(f"  æœªæ‰¾åˆ°æ¨¡å‹: {model_name}")
    
    # 4. æ›´æ–°æ¨¡å‹çŠ¶æ€
    print("\n4. æ›´æ–°æ¨¡å‹çŠ¶æ€:")
    model_registry.update_model_status("YOLOv5-Small", "running", "cuda:0")
    model_registry.update_model_status("ResNet18", "running", "cpu")
    model_registry.update_model_status("EfficientNet", "error")
    
    print("  çŠ¶æ€æ›´æ–°åçš„æ¨¡å‹åˆ—è¡¨:")
    models = model_registry.list_models()
    for model in models:
        status_emoji = {
            'running': 'ğŸŸ¢',
            'stopped': 'âšª',
            'error': 'ğŸ”´'
        }.get(model['status'], 'â“')
        print(f"  {status_emoji} {model['name']} - {model['status']}")
        if model['device']:
            print(f"      è®¾å¤‡: {model['device']}")
        if model['loaded_time']:
            print(f"      åŠ è½½æ—¶é—´: {model['loaded_time']}")


def test_system_info():
    """æµ‹è¯•ç³»ç»Ÿä¿¡æ¯è·å–"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç³»ç»Ÿä¿¡æ¯è·å–")
    print("=" * 60)
    
    # 1. è·å–ç³»ç»ŸåŸºæœ¬ä¿¡æ¯
    print("\n1. ç³»ç»ŸåŸºæœ¬ä¿¡æ¯:")
    sys_info = get_system_info()
    print(f"  æ“ä½œç³»ç»Ÿ: {sys_info['platform']} {sys_info['platform_release']}")
    print(f"  æ¶æ„: {sys_info['architecture']}")
    print(f"  å¤„ç†å™¨: {sys_info['processor']}")
    print(f"  Pythonç‰ˆæœ¬: {sys_info['python_version'].split()[0]}")
    print(f"  PyTorchç‰ˆæœ¬: {sys_info['torch_version']}")
    print(f"  CUDAå¯ç”¨: {sys_info['cuda_available']}")
    if sys_info['cuda_version']:
        print(f"  CUDAç‰ˆæœ¬: {sys_info['cuda_version']}")


def test_resource_monitoring():
    """æµ‹è¯•èµ„æºç›‘æ§åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•èµ„æºç›‘æ§åŠŸèƒ½")
    print("=" * 60)
    
    # 1. è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
    print("\n1. å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ:")
    usage = get_resource_usage()
    
    print(f"  æ—¶é—´æˆ³: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(usage['timestamp']))}")
    
    # CPUä¿¡æ¯
    cpu = usage['cpu']
    print(f"  CPUä½¿ç”¨ç‡: {cpu['percent']:.1f}%")
    print(f"  CPUæ ¸å¿ƒæ•°: {cpu['count']}")
    print(f"  CPUé¢‘ç‡: {cpu['frequency']:.0f} MHz")
    
    # å†…å­˜ä¿¡æ¯
    memory = usage['memory']
    print(f"  å†…å­˜ä½¿ç”¨ç‡: {memory['percent']:.1f}%")
    print(f"  æ€»å†…å­˜: {memory['total'] / (1024**3):.2f} GB")
    print(f"  å·²ç”¨å†…å­˜: {memory['used'] / (1024**3):.2f} GB")
    print(f"  å¯ç”¨å†…å­˜: {memory['available'] / (1024**3):.2f} GB")
    
    # GPUä¿¡æ¯
    if 'gpu' in usage:
        gpu = usage['gpu']
        print(f"  GPUå¯ç”¨: {gpu.get('cuda_available', False)}")
        if 'devices' in gpu:
            for i, device in enumerate(gpu['devices']):
                print(f"  GPU {i}: {device.get('name', 'Unknown')}")
                if 'utilization' in device:
                    print(f"    ä½¿ç”¨ç‡: {device['utilization']:.1f}%")
                    print(f"    å†…å­˜ä½¿ç”¨ç‡: {device['memory_util']:.1f}%")
                if 'temperature' in device and device['temperature']:
                    print(f"    æ¸©åº¦: {device['temperature']}Â°C")
    else:
        print("  æœªæ£€æµ‹åˆ°GPUæˆ–GPUä¿¡æ¯è·å–å¤±è´¥")


def test_gpu_info():
    """æµ‹è¯•GPUä¿¡æ¯è·å–"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•GPUä¿¡æ¯è·å–")
    print("=" * 60)
    
    gpu_info = get_gpu_info()
    if gpu_info:
        print(f"  CUDAå¯ç”¨: {gpu_info.get('cuda_available', False)}")
        print(f"  è®¾å¤‡æ•°é‡: {gpu_info.get('device_count', 0)}")
        
        if 'devices' in gpu_info:
            for device in gpu_info['devices']:
                print(f"\n  GPU {device['index']}:")
                print(f"    åç§°: {device.get('name', 'Unknown')}")
                if 'total_memory' in device:
                    print(f"    æ˜¾å­˜: {device['total_memory'] / (1024**3):.2f} GB")
                if 'utilization' in device:
                    print(f"    ä½¿ç”¨ç‡: {device['utilization']}%")
                    print(f"    æ˜¾å­˜ä½¿ç”¨ç‡: {device['memory_util']}%")
                if 'memory_used' in device:
                    print(f"    å·²ç”¨æ˜¾å­˜: {device['memory_used'] / (1024**3):.2f} GB")
                    print(f"    å¯ç”¨æ˜¾å­˜: {device['memory_free'] / (1024**3):.2f} GB")
                if 'temperature' in device and device['temperature']:
                    print(f"    æ¸©åº¦: {device['temperature']}Â°C")
    else:
        print("  æœªæ£€æµ‹åˆ°GPUä¿¡æ¯")


def test_model_statistics():
    """æµ‹è¯•æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯è·å–"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯è·å–")
    print("=" * 60)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 32, 3, 1, 1)
            self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc1 = nn.Linear(64, 128)
            self.fc2 = nn.Linear(128, 10)
            self.dropout = nn.Dropout(0.5)
            
        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = torch.relu(self.conv2(x))
            x = self.pool(x)
            x = x.view(x.size(0), -1)
            x = torch.relu(self.fc1(x))
            x = self.dropout(x)
            x = self.fc2(x)
            return x
    
    model = SimpleModel()
    
    print("\n1. ç®€å•CNNæ¨¡å‹ç»Ÿè®¡:")
    stats = get_model_statistics(model)
    
    if 'error' in stats:
        print(f"  é”™è¯¯: {stats['error']}")
    else:
        print(f"  æ€»å‚æ•°é‡: {stats['total_parameters']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {stats['trainable_parameters']:,}")
        print(f"  ä¸å¯è®­ç»ƒå‚æ•°: {stats['non_trainable_parameters']:,}")
        print(f"  æ¨¡å‹å¤§å°: {stats['model_size_mb']:.2f} MB")
        print(f"  å±‚æ•°: {stats['layers_count']}")
        
        print("\n  å‰å‡ å±‚è¯¦ç»†ä¿¡æ¯:")
        for layer in stats['layers_info'][:5]:
            print(f"    {layer['name']} ({layer['type']}): {layer['params']:,} å‚æ•°")


def test_stream_monitoring():
    """æµ‹è¯•æµå¼èµ„æºç›‘æ§"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æµå¼èµ„æºç›‘æ§ (è¿è¡Œ5ç§’)")
    print("=" * 60)
    
    print("\nå®æ—¶ç›‘æ§æ•°æ®:")
    print("æ—¶é—´æˆ³               CPU%   å†…å­˜%   GPU%")
    print("-" * 50)
    
    count = 0
    for usage_data in resource_monitor_stream(interval=1.0):
        if 'error' in usage_data:
            print(f"é”™è¯¯: {usage_data['error']}")
            break
            
        timestamp = time.strftime('%H:%M:%S', time.localtime(usage_data['timestamp']))
        cpu_percent = usage_data['cpu']['percent']
        memory_percent = usage_data['memory']['percent']
        
        gpu_percent = "N/A"
        if 'gpu' in usage_data and usage_data['gpu']['devices']:
            gpu_percent = f"{usage_data['gpu']['devices'][0].get('utilization', 0)}"
        
        print(f"{timestamp}        {cpu_percent:5.1f}  {memory_percent:5.1f}  {gpu_percent:>4}")
        
        count += 1
        if count >= 5:  # åªè¿è¡Œ5æ¬¡
            break


def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½")
    print("=" * 60)
    
    print("\n1. æµ‹è¯•ä¸å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶:")
    model = load_model_from_path("/path/to/nonexistent/model.pth")
    print(f"  ç»“æœ: {model}")
    
    print("\n2. åˆ›å»ºå¹¶ä¿å­˜ä¸€ä¸ªç®€å•æ¨¡å‹:")
    # åˆ›å»ºä¸€ä¸ªç®€å•æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(10, 1)
            
        def forward(self, x):
            return self.linear(x)
    
    test_model = TestModel()
    
    # ä¿å­˜æ¨¡å‹åˆ°ä¸´æ—¶ä½ç½®
    temp_path = "temp_model.pth"
    torch.save(test_model.state_dict(), temp_path)
    print(f"  æ¨¡å‹å·²ä¿å­˜åˆ°: {temp_path}")
    
    # å°è¯•åŠ è½½æ¨¡å‹
    print("\n3. åŠ è½½ä¿å­˜çš„æ¨¡å‹:")
    try:
        loaded_model = TestModel()
        state_dict = torch.load(temp_path, map_location='cpu')
        loaded_model.load_state_dict(state_dict)
        print("  æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
        stats = get_model_statistics(loaded_model)
        print(f"  å‚æ•°é‡: {stats['total_parameters']}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_path)
        print("  ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        
    except Exception as e:
        print(f"  æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("CommonæœåŠ¡æ¨¡å—åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    
    try:
        test_model_registry()
        test_system_info()
        test_resource_monitoring()
        test_gpu_info()
        test_model_statistics()
        test_stream_monitoring()
        test_model_loading()
        
        print("\n" + "=" * 80)
        print("æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 